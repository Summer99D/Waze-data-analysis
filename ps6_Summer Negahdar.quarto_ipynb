{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"Problem Set 6 - Waze Shiny Dashboard\"\n",
        "author: \"Summer Negahdar\"\n",
        "date: today\n",
        "format: \n",
        "  pdf:\n",
        "    include-in-header: \n",
        "       text: |\n",
        "         \\usepackage{fvextra}\n",
        "         \\DefineVerbatimEnvironment{Highlighting}{Verbatim}{breaklines,commandchars=\\\\\\{\\}}\n",
        "include-before-body:\n",
        "  text: |\n",
        "    \\RecustomVerbatimEnvironment{verbatim}{Verbatim}{\n",
        "      showspaces = false,\n",
        "      showtabs = false,\n",
        "      breaksymbolleft={},\n",
        "      breaklines\n",
        "    }\n",
        "---\n",
        "\n",
        "1. **ps6:** Due Sat 23rd at 5:00PM Central. Worth 100 points (80 points from questions, 10 points for correct submission and 10 points for code style) + 10 extra credit. \n",
        "\n",
        "We use (`*`) to indicate a problem that we think might be time consuming. \n",
        "\n",
        "# Steps to submit (10 points on PS6) {-}\n",
        "\n",
        "1. \"This submission is my work alone and complies with the 30538 integrity\n",
        "policy.\" Add your initials to indicate your agreement: SN\n",
        "2. \"I have uploaded the names of anyone I worked with on the problem set \n",
        "3. Late coins used this pset: 1 Late coins left after submission: 00\n",
        "\n",
        "\n",
        "*IMPORTANT: For the App portion of the PS, in case you can not arrive to the expected functional dashboard we will need to take a look at your `app.py` file. You can use the following code chunk template to \"import\" and print the content of that file. Please, don't forget to also tag the corresponding code chunk as part of your submission!*\n"
      ],
      "id": "fffd807d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: true\n",
        "#| eval: false\n",
        "\n",
        "def print_file_contents(file_path):\n",
        "    \"\"\"Print contents of a file.\"\"\"\n",
        "    try:\n",
        "        with open(file_path, 'r') as f:\n",
        "            content = f.read()\n",
        "            print(\"```python\")\n",
        "            print(content)\n",
        "            print(\"```\")\n",
        "    except FileNotFoundError:\n",
        "        print(\"```python\")\n",
        "        print(f\"Error: File '{file_path}' not found\")\n",
        "        print(\"```\")\n",
        "    except Exception as e:\n",
        "        print(\"```python\") \n",
        "        print(f\"Error reading file: {e}\")\n",
        "        print(\"```\")\n",
        "\n",
        "print_file_contents(\"/Users/samarnegahdar/Desktop/untitled folder/pset-VI/shiny app/Q5_a/app.py\")"
      ],
      "id": "2edc56a6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Import required packages.\n",
        "import pandas as pd\n",
        "from datetime import date\n",
        "import numpy as np\n",
        "import altair as alt\n",
        "alt.data_transformers.disable_max_rows() \n",
        "\n",
        "import json"
      ],
      "id": "05638070",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Background {-}\n",
        "\n",
        "## Data Download and Exploration (20 points){-} \n",
        "\n",
        "1. \n"
      ],
      "id": "cc9850e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Import required packages.\n",
        "import pandas as pd\n",
        "import altair as alt \n",
        "from datetime import date\n",
        "import numpy as np\n",
        "alt.data_transformers.disable_max_rows() \n",
        "\n",
        "import json\n",
        "Waze= pd.read_csv('/Users/samarnegahdar/Desktop/untitled folder/pset-VI/waze_data/waze_data.csv')\n",
        "\n",
        "Waze_df= pd.DataFrame(Waze)\n",
        "\n",
        "##defining data types in altair syntax system\n",
        "\n",
        "def map_to_altair_type(dtype):\n",
        "    if pd.api.types.is_numeric_dtype(dtype):\n",
        "        return \"Q\"\n",
        "    elif pd.api.types.is_datetime64_any_dtype(dtype):\n",
        "        return \"T\"\n",
        "    elif pd.api.types.is_bool_dtype(dtype):\n",
        "        return \"N\"\n",
        "    elif pd.api.types.is_categorical_dtype(dtype):\n",
        "        return \"O\"\n",
        "    else:\n",
        "        return \"N\"\n",
        "\n",
        "##making a subset to ignor the three columns\n",
        "Q1_subset = Waze_df.drop(columns=['geo', 'ts', 'geoWKT'])\n",
        "##Assigning data types based on Altair syntax\n",
        "altair_types_report = pd.DataFrame({\n",
        "    \"Column Name\": Q1_subset.columns,\n",
        "    \"Altair Data Type\": [map_to_altair_type(Q1_subset[col]) for col in Q1_subset.columns]\n",
        "})\n",
        "\n",
        "print(altair_types_report)"
      ],
      "id": "8c313f32",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n"
      ],
      "id": "8197012b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "##summing up NAs\n",
        "missing_counts = Waze_df.isnull().sum()\n",
        "not_missing_counts = Waze_df.notnull().sum()\n",
        "\n",
        "missing_summary = pd.DataFrame({\n",
        "    'Variable': Waze_df.columns,\n",
        "    'Missing': missing_counts,\n",
        "    'Not Missing': not_missing_counts\n",
        "})\n",
        "\n",
        "# Melt the DataFrame to make it suitable for Altair\n",
        "melted_data = missing_summary.melt(\n",
        "    id_vars='Variable', \n",
        "    value_vars=['Missing', 'Not Missing'], \n",
        "    var_name='Status', \n",
        "    value_name='Count'\n",
        ")\n",
        "\n",
        "# Step 3: Plot the stacked bar chart\n",
        "Q2_stack_chart = alt.Chart(melted_data).mark_bar().encode(\n",
        "    x=alt.X('Variable:N', title='Variables',axis=alt.Axis(labelAngle=45)),\n",
        "    y=alt.Y('Count:Q', title='Number of Observations'),\n",
        "    color=alt.Color('Status:N', title='Status', scale=alt.Scale(domain=['Missing', 'Not Missing'], range=['red', 'green'])),\n",
        "    tooltip=['Variable', 'Status', 'Count']\n",
        ").properties(\n",
        "    title='NA vs non-NA Observations by Variable',\n",
        "    width=800,\n",
        "    height=400\n",
        ")\n",
        "Q2_stack_chart.save('Q2_stack_chart.png', scale=2)"
      ],
      "id": "887419b5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Stack bar chart](Q2_stack_chart.png)\n"
      ],
      "id": "a95d1383"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 4: Analyze variables with missing values\n",
        "# Find variables with missing values\n",
        "variables_with_missing = missing_summary[missing_summary['Missing'] > 0]\n",
        "\n",
        "# Find the variable with the highest share of missing values\n",
        "missing_summary['Missing Share'] = missing_summary['Missing'] / (missing_summary['Missing'] + missing_summary['Not Missing'])\n",
        "variable_highest_missing = missing_summary.loc[missing_summary['Missing Share'].idxmax()]\n",
        "\n",
        "print(\"Variables with missing values:\")\n",
        "print(variables_with_missing)\n",
        "\n",
        "print(\"\\nVariable with the highest share of missing values:\")\n",
        "print(variable_highest_missing)"
      ],
      "id": "2d12ea31",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "3. \n",
        "## a.\n",
        "Print the unique values for the columns type and subtype. How many types have a subtype that is NA?\n"
      ],
      "id": "c67e8540"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "unique_types = Waze_df['type'].unique()\n",
        "unique_subtypes = Waze_df['subtype'].unique()\n",
        "\n",
        "print(f\"Unique 'type' values: {unique_types}\")\n",
        "print(f\"Unique 'subtype' values: {unique_subtypes}\")\n",
        "\n",
        "##we need to group variables based on Type column and find those whose subtype is NA\n",
        "\n",
        "missing_subtype_counts = Waze_df[Waze_df['subtype'].isna()].groupby('type').size()\n",
        "\n",
        "print(\"Count of types with NA subtypes:\")\n",
        "print(missing_subtype_counts)\n",
        "\n",
        "# Total number of types with at least one NA subtype\n",
        "types_with_na_subtype = missing_subtype_counts.index.nunique()\n",
        "print(f\"Number of types with at least one NA subtype: {types_with_na_subtype}\")"
      ],
      "id": "e12e1e2c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## b. \n",
        "Printing all the unique combos of type and subtype, we have 24 unique combos. \n"
      ],
      "id": "9c01048b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "unique_combinations = Waze_df[['type', 'subtype']].drop_duplicates().reset_index().drop(columns= 'index')\n",
        "print(unique_combinations)\n",
        "\n",
        "detailed_types = unique_combinations.groupby('type')['subtype'].nunique()\n",
        "print(\"Number of unique subtypes per type:\")\n",
        "print(detailed_types)\n",
        "# to see which ones can have a sub-subtype, it means that there sohuld be more than one type and subtype combo for a specific subtype. "
      ],
      "id": "f92d3f8d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# I want to see how many different variations of subtype for each type I have\n",
        "unique_subtypes_per_type = Waze_df.groupby('type')['subtype'].unique()\n",
        "\n",
        "# Display the unique subtypes for each type\n",
        "for type_value, subtypes in unique_subtypes_per_type.items():\n",
        "    print(f\"Type: {type_value}\")\n",
        "    print(f\"Unique Subtypes: {list(subtypes)}\")\n",
        "    print(\"-\" * 50)"
      ],
      "id": "a34151aa",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## c.\n",
        "Accident(t)> \n",
        "  Major\n",
        "  Minor \n",
        "  Nan\n",
        "\n",
        "Road_closed(t)> \n",
        "  Event \n",
        "  Nan\n",
        "\n",
        "Hazard(t)> \n",
        "  shoulder \n",
        "  road:\n",
        "    construction, \n",
        "    car, \n",
        "    emergency, \n",
        "    traffic, \n",
        "    pothole,\n",
        "    object,\n",
        "    lane_closure,\n",
        "    road kill\n",
        "  weather:\n",
        "    snow, \n",
        "    fog, \n",
        "    flood\n",
        "\n",
        "Jam(t)>\n",
        "  heavy\n",
        "  standstill\n",
        "  moderate\n",
        "we can agree that HAZARD has enough subtypes with information that can have a new sub-subtype,\n",
        "also, JAM can have sub-subtype if we change the type to \"traffic\"\n",
        "\n",
        "## d.\n"
      ],
      "id": "c9c0fdac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# I want to see how many NAs I have in the subtype column\n",
        "na_subtype_count = Waze_df['subtype'].isna().sum()\n",
        "ratio_of_subtype= na_subtype_count/ len(Waze_df)\n",
        "\n",
        "print(f\"The number of NA subtypes is: {na_subtype_count}\")\n",
        "print(ratio_of_subtype)"
      ],
      "id": "055c0699",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "there are 96k subtype and which is almost 1/8th(12%) of our variables and I dont think removing them is smart. that is why I will change all of it to \"unclassified\"\n"
      ],
      "id": "48609c94"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "Waze_df['subtype'] = Waze_df['subtype'].fillna('Unclassified')"
      ],
      "id": "98c55571",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \n",
        "\n",
        "## a. "
      ],
      "id": "574e663b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 1: Extract unique combinations of type and subtype from Waze_df\n",
        "crosswalk_df = Waze_df[['type', 'subtype']].drop_duplicates().reset_index(drop=True)"
      ],
      "id": "ab5ad07a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## b. \n"
      ],
      "id": "e278b871"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def assign_updated_subtype(row):\n",
        "    \"\"\"\n",
        "    Categorize HAZARD subtypes into 'Road', 'Shoulder', or 'Weather'.\n",
        "    Handle JAM and other types accordingly.\n",
        "    \"\"\"\n",
        "    if row['type'] == 'HAZARD':\n",
        "        if 'HAZARD_ON_SHOULDER' in str(row['subtype']).upper():\n",
        "            return 'Shoulder'\n",
        "        elif 'HAZARD_ON_ROAD' in str(row['subtype']).upper():\n",
        "            return 'Road'\n",
        "        elif 'HAZARD_WEATHER' in str(row['subtype']).upper():\n",
        "            return 'Weather'\n",
        "        else:\n",
        "            return 'Unclassified'\n",
        "    elif row['type'] == 'ACCIDENT':\n",
        "        if 'ACCIDENT_MAJOR' in str(row['subtype']).upper():\n",
        "            return 'Major'\n",
        "        elif 'ACCIDENT_MINOR' in str(row['subtype']).upper():\n",
        "            return 'Minor'\n",
        "        else:\n",
        "            return 'Unclassified'\n",
        "    elif row['type'] == 'ROAD_CLOSED':\n",
        "        if 'EVENT' in str(row['subtype']).upper():\n",
        "            return 'Event'\n",
        "        else:\n",
        "            return 'Unclassified'\n",
        "    elif row['type'] == 'JAM':\n",
        "        return 'Traffic'\n",
        "    else:\n",
        "        return 'Unclassified'\n",
        "\n",
        "def assign_updated_subsubtype(row):\n",
        "    \"\"\"\n",
        "    Assign sub-subcategories for 'Road', 'Shoulder', and 'Weather' in HAZARD, \n",
        "    and for JAM subtypes.\n",
        "    \"\"\"\n",
        "    if row['type'] == 'HAZARD' and row['updated_subtype'] == 'Road':\n",
        "        if 'CONSTRUCTION' in str(row['subtype']).upper():\n",
        "            return 'Construction'\n",
        "        elif 'CAR_STOPPED' in str(row['subtype']).upper():\n",
        "            return 'Car Stopped'\n",
        "        elif 'EMERGENCY' in str(row['subtype']).upper():\n",
        "            return 'Emergency Vehicle'\n",
        "        elif 'TRAFFIC_LIGHT' in str(row['subtype']).upper():\n",
        "            return 'Traffic Light Fault'\n",
        "        elif 'POT_HOLE' in str(row['subtype']).upper():\n",
        "            return 'Pothole'\n",
        "        elif 'OBJECT' in str(row['subtype']).upper():\n",
        "            return 'Object'\n",
        "        elif 'LANE_CLOSED' in str(row['subtype']).upper():\n",
        "            return 'Lane Closed'\n",
        "        elif 'ROAD_KILL' in str(row['subtype']).upper():\n",
        "            return 'Road Kill'\n",
        "        else:\n",
        "            return 'Unclassified'\n",
        "    elif row['type'] == 'HAZARD' and row['updated_subtype'] == 'Weather':\n",
        "        if 'SNOW' in str(row['subtype']).upper():\n",
        "            return 'Snow'\n",
        "        elif 'FOG' in str(row['subtype']).upper():\n",
        "            return 'Fog'\n",
        "        elif 'FLOOD' in str(row['subtype']).upper():\n",
        "            return 'Flood'\n",
        "        else:\n",
        "            return 'Unclassified'\n",
        "    elif row['type'] == 'HAZARD' and row['updated_subtype'] == 'Shoulder':\n",
        "        if 'CAR_STOPPED' in str(row['subtype']).upper():\n",
        "            return 'Car Stopped'\n",
        "        else:\n",
        "            return 'Unclassified'\n",
        "    elif row['type'] == 'JAM':\n",
        "        if 'HEAVY_TRAFFIC' in str(row['subtype']).upper():\n",
        "            return 'Heavy'\n",
        "        elif 'MODERATE_TRAFFIC' in str(row['subtype']).upper():\n",
        "            return 'Moderate'\n",
        "        elif 'STAND_STILL_TRAFFIC' in str(row['subtype']).upper():\n",
        "            return 'Standstill'\n",
        "        elif 'LIGHT_TRAFFIC' in str(row['subtype']).upper():\n",
        "            return 'Light'\n",
        "        else:\n",
        "            return 'Unclassified'\n",
        "    elif row['type'] == 'ACCIDENT':\n",
        "        return row['updated_subtype']  # Keep Major/Minor as the sub-subtype\n",
        "    elif row['type'] == 'ROAD_CLOSED':\n",
        "        return row['updated_subtype']  # Keep Event as the sub-subtype\n",
        "    return 'Unclassified'\n",
        "\n",
        "# Assign updated_type to crosswalk\n",
        "crosswalk_df['updated_type'] = crosswalk_df['type'].str.capitalize()\n",
        "\n",
        "# Apply updated_subtype logic to crosswalk\n",
        "crosswalk_df['updated_subtype'] = crosswalk_df.apply(assign_updated_subtype, axis=1)\n",
        "\n",
        "# Apply updated_subsubtype logic to crosswalk\n",
        "crosswalk_df['updated_subsubtype'] = crosswalk_df.apply(assign_updated_subsubtype, axis=1)\n",
        "\n",
        "# Verify the updated crosswalk\n",
        "print(\"Updated Crosswalk Table:\")\n",
        "print(crosswalk_df)\n",
        "\n",
        "# Save the crosswalk table for reference\n",
        "crosswalk_df.to_csv(\"crosswalk_table.csv\", index=False)\n",
        "print(\"Crosswalk table saved as 'crosswalk_table.csv'\")"
      ],
      "id": "8cff04fb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## c. \n"
      ],
      "id": "22825061"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Merge the crosswalk with the original data\n",
        "Waze_merged_df = Waze_df.merge(\n",
        "    crosswalk_df,\n",
        "    on=['type', 'subtype'],\n",
        "    how='left'\n",
        ")\n",
        "\n",
        "# Check for rows where type is 'Accident' and subtype is 'Unclassified'\n",
        "accident_unclassified_count = Waze_merged_df[\n",
        "    (Waze_merged_df['type'] == 'ACCIDENT') &\n",
        "    (Waze_merged_df['subtype'] == 'Unclassified')\n",
        "].shape[0]\n",
        "\n",
        "# Display the count\n",
        "print(f\"Number of rows for Accident - Unclassified: {accident_unclassified_count}\")"
      ],
      "id": "59a97272",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## d. \n"
      ],
      "id": "75a4544b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Step 2: Check consistency between 'type' and 'subtype' in crosswalk_df and merged_df\n",
        "crosswalk_types = crosswalk_df[['type', 'subtype']].drop_duplicates()\n",
        "merged_types = Waze_merged_df[['type', 'subtype']].drop_duplicates()\n",
        "\n",
        "# Step 3: Verify that all combinations in crosswalk are in merged_df\n",
        "missing_in_merged = crosswalk_types.merge(\n",
        "    merged_types,\n",
        "    on=['type', 'subtype'],\n",
        "    how='left',\n",
        "    indicator=True\n",
        ").query(\"_merge == 'left_only'\")\n",
        "\n",
        "# Print results\n",
        "if missing_in_merged.empty:\n",
        "    print(\"All type and subtype combinations in the crosswalk are present in the merged dataset.\")\n",
        "else:\n",
        "    print(\"The following type and subtype combinations in the crosswalk are missing in the merged dataset:\")\n",
        "    print(missing_in_merged[['type', 'subtype']])"
      ],
      "id": "18029786",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# App #1: Top Location by Alert Type Dashboard (30 points){-}\n",
        "\n",
        "1. \n",
        "\n",
        "a. "
      ],
      "id": "d1d40fab"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import re\n",
        "\n",
        "def extract_coordinates(geo_string):\n",
        "    if pd.notna(geo_string):  # Ensure the string is not NaN\n",
        "        match = re.search(r\"POINT\\(([-\\d.]+) ([-\\d.]+)\\)\", geo_string)\n",
        "        if match:\n",
        "            return float(match.group(1)), float(match.group(2))\n",
        "    return None, None\n",
        "\n",
        "# Apply the function to extract latitude and longitude\n",
        "Waze_merged_df[\"longitude\"], Waze_merged_df[\"latitude\"] = zip(*Waze_merged_df[\"geo\"].apply(extract_coordinates))\n",
        "\n",
        "# Display the updated DataFrame\n",
        "print(Waze_merged_df.head(5))"
      ],
      "id": "b7e0bbc6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. "
      ],
      "id": "cf9fafde"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure latitude and longitude are numeric\n",
        "Waze_merged_df['latitude'] = pd.to_numeric(Waze_merged_df['latitude'], errors='coerce')\n",
        "Waze_merged_df['longitude'] = pd.to_numeric(Waze_merged_df['longitude'], errors='coerce')\n",
        "\n",
        "# Bin latitude and longitude with a step size of 0.01\n",
        "Waze_merged_df['binned_latitude'] = (Waze_merged_df['latitude'] // 0.01 * 0.01).round(2)\n",
        "Waze_merged_df['binned_longitude'] = (Waze_merged_df['longitude'] // 0.01 * 0.01).round(2)\n",
        "\n",
        "# Combine the binned latitude and longitude into a single column for unique combinations\n",
        "Waze_merged_df['binned_lat_lon'] = list(zip(Waze_merged_df['binned_latitude'], Waze_merged_df['binned_longitude']))\n",
        "\n",
        "# Count occurrences of each binned latitude-longitude combination\n",
        "binned_counts = Waze_merged_df['binned_lat_lon'].value_counts().reset_index()\n",
        "binned_counts.columns = ['binned_lat_lon', 'count']\n",
        "\n",
        "# Find the combination with the greatest number of observations\n",
        "most_common_bin = binned_counts.iloc[0]\n",
        "print(f\"Most frequent binned latitude-longitude: {most_common_bin['binned_lat_lon']} with {most_common_bin['count']} observations\")\n",
        "\n",
        "# Create the 'type_subsubtype' column by combining 'updated_type' and 'updated_subtype'\n",
        "Waze_merged_df['type_subtype'] = Waze_merged_df['updated_type'] + \" - \" + Waze_merged_df['updated_subtype']\n",
        "\n",
        "##now I want to save Waze_merged_df as a csv so I can use it later for my shiny app!!\n",
        "Waze_merged_df.to_csv('/Users/samarnegahdar/Desktop/untitled folder/pset-VI/Waze_merged_data.csv', index=False)"
      ],
      "id": "b6b759a5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. "
      ],
      "id": "7eaa9601"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# I want to filter data for Hazard due to \"snow\"\n",
        " \n",
        "Q2_1c = Waze_merged_df[\n",
        "    (Waze_merged_df['type'] == 'HAZARD') &\n",
        "    (Waze_merged_df['updated_subsubtype'] == 'Snow')\n",
        "]\n",
        "\n",
        "# Aggregate data at the binned latitude-longitude level\n",
        "top_alerts_map = (\n",
        "    Q2_1c.groupby('binned_lat_lon')\n",
        "    .size()\n",
        "    .reset_index(name='alert_count')\n",
        "    .sort_values(by='alert_count', ascending=False)\n",
        "    .head(10)  # Top 10 bins\n",
        ")\n",
        "\n",
        "print(f\"Level of Aggregation: Binned latitude-longitude\")\n",
        "print(f\"Number of Rows: {len(top_alerts_map)}\")\n",
        "print(top_alerts_map)"
      ],
      "id": "89c73f7d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2.   \n",
        "a. \n"
      ],
      "id": "400c0c02"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Using 'updated_type' and 'updated_subsubtype' for filtering\n",
        "jam_heavy_df = Waze_merged_df[\n",
        "    (Waze_merged_df['updated_type'].str.contains('jam', case=False, na=False)) &  # Case insensitive filtering\n",
        "    (Waze_merged_df['updated_subsubtype'].str.contains('heavy', case=False, na=False))  # Case insensitive filtering\n",
        "]\n",
        "\n",
        "# Check if any data exists after filtering\n",
        "print(jam_heavy_df.shape)  # Check number of rows after filtering\n",
        "print(jam_heavy_df[['updated_type', 'updated_subsubtype', 'binned_lat_lon']].head())  # Check the first few rows\n",
        "\n",
        "#Aggregate the data to find the number of alerts for each binned_lat_lon\n",
        "top_jam_heavy = (\n",
        "    jam_heavy_df.groupby('binned_lat_lon')\n",
        "    .size()\n",
        "    .reset_index(name='alert_count')\n",
        "    .sort_values(by='alert_count', ascending=False)\n",
        "    .head(10)\n",
        ")\n",
        "\n",
        "\n",
        "top_jam_heavy['binned_lat_lon'] = top_jam_heavy['binned_lat_lon'].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x)\n",
        "\n",
        "# Step 4: Split the binned_lat_lon into latitude and longitude columns\n",
        "top_jam_heavy[['latitude', 'longitude']] = pd.DataFrame(\n",
        "    top_jam_heavy['binned_lat_lon'].tolist(), \n",
        "    index=top_jam_heavy.index\n",
        ")\n",
        "\n",
        "print(top_jam_heavy.head())\n",
        "\n",
        "#Create the scatter plot using Altair\n",
        "scatter_plot_Jam = alt.Chart(top_jam_heavy).mark_circle().encode(\n",
        "    x=alt.X('longitude:Q', title='Longitude', scale=alt.Scale(domain=[-87.8, -87.4])),\n",
        "    y=alt.Y('latitude:Q', title='Latitude', scale=alt.Scale(domain=[41.8, 42.0])),\n",
        "    size=alt.Size('alert_count:Q', title='Number of Alerts', legend=alt.Legend(title=\"Alert Count\")),\n",
        "    tooltip=['latitude:Q', 'longitude:Q', 'alert_count:Q']\n",
        ").properties(\n",
        "    title='Top 10 Locations with Most Jam - Heavy Traffic Alerts',\n",
        "    width=600,\n",
        "    height=400\n",
        ").project(type=\"identity\", reflectY=True)  # Apply the same Mercator projection to the scatter plot\n",
        "\n",
        "scatter_plot_Jam.save('Heavy_traffic_scatterplot.png', scale=2)"
      ],
      "id": "edc9d5a3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Heavy traffic scatterplot](Heavy_traffic_scatterplot.png)\n",
        "\n",
        "\n",
        "## 3.\n"
      ],
      "id": "29c0d2a9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import requests\n",
        "\n",
        "# Send a GET request to the URL\n",
        "response = requests.get(\"https://data.cityofchicago.org/api/geospatial/bbvz-uum9?method=export&format=GeoJSON\")"
      ],
      "id": "45167638",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "b. "
      ],
      "id": "73b2d44c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import altair as alt\n",
        "import pandas as pd\n",
        "import json\n",
        "\n",
        "# Load the GeoJSON data\n",
        "geojson_file = '/Users/samarnegahdar/Desktop/untitled folder/pset-VI/chicago_neighborhoods.geojson'\n",
        "with open(geojson_file) as f:\n",
        "    chicago_geojson = json.load(f)\n",
        "\n",
        "# Convert GeoJSON to Altair's data format\n",
        "geo_data = alt.Data(values=chicago_geojson[\"features\"])"
      ],
      "id": "a4b605f6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "4. \n"
      ],
      "id": "0fd4dabc"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Define latitude and longitude ranges (domain)\n",
        "lat_range = [41.8, 42.0]  \n",
        "lon_range = [-87.8, -87.4]  \n",
        "\n",
        "# Base map using GeoJSON with specific latitude and longitude ranges\n",
        "base_map = alt.Chart(geo_data).mark_geoshape(\n",
        "    fill='lightgray',  # Fill color for the map\n",
        "    stroke='white'     # Border color for neighborhoods\n",
        ").project(\n",
        "    type='mercator',\n",
        "    scale=100000,  # Adjust scale to zoom in to the specific region\n",
        "    center=[-87.58, 41.9],  # Center the map on Chicago's approximate coordinates\n",
        ").properties(\n",
        "    width=600,\n",
        "    height=400\n",
        ").encode(\n",
        "    longitude='longitude:Q',\n",
        "    latitude='latitude:Q'\n",
        ")\n",
        "\n",
        "# Adjust the base map's latitude and longitude range (domain)\n",
        "base_map = base_map.encode(\n",
        "    x=alt.X('longitude:Q', scale=alt.Scale(domain=lon_range)),  # Longitude domain\n",
        "    y=alt.Y('latitude:Q', scale=alt.Scale(domain=lat_range))    # Latitude domain\n",
        ")\n",
        "\n",
        "# Display the base map\n",
        "base_map.show()\n",
        "\n",
        "# Layer the scatter plot on top of the base map\n",
        "layered_chart_Jam = base_map + scatter_plot_Jam\n",
        "\n",
        "# Display the final layered chart\n",
        "layered_chart_Jam.save('Jam_heavy_map.png', scale=2)"
      ],
      "id": "490e127e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![heavy Jam layered map](Jam_heavy_map.png)\n",
        "\n",
        "5. \n",
        "a.\n",
        "there are 22 options for the dropdown menu\n",
        "![drop_down menu count](dashboard_app1.png){width=70% align=\"center\"}\n",
        "\n",
        "\n",
        "b. \n",
        "\n",
        "![Heavy Jam traffic alerts](Jam_heavy.png){width=70% align=\"center\"}\n",
        "c. \n",
        "mostly in bucktown and chinatown and westloop. (which makes sense considering all the events take place in these three locations)\n",
        "![Event-related road closure alerts](event_road.png){width=70% align=\"center\"}\n",
        "\n",
        "\n",
        "d. \n",
        "are there more major accidents in downtown or minor accidents?\n",
        "there are mor major accidents than minor accidents in downtown (which is interesting since there is a speed limit inside the city vs. on highways/freeways)\n",
        "\n",
        "![Major Accident alerts](Major_accident.png){width=70% align=\"center\"}\n",
        "![Minor Accident alerts](Minor_accident.png){width=70% align=\"center\"}\n",
        "\n",
        "e. \n",
        "I would say the most crucial thing to add is time frame, either annual or daily (like which times of the year which events happen or which times of the day certain alerts are more frequent)\n",
        "\n",
        "# App #2: Top Location by Alert Type and Hour Dashboard (20 points) {-}\n",
        "\n",
        "1. \n",
        "\n",
        "a. \n",
        "I would say using the raw data from ts is not smart as it will give us too many bins based on each day. what we want to to have an accumulated set of data for each hour across time (days,weeks,...) so its better to limit the categories if we want the user to choose a specific time.\n",
        "    \n",
        "b. \n"
      ],
      "id": "7f088fa7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Ensure the 'ts' column is in datetime format\n",
        "Waze_merged_df['ts'] = pd.to_datetime(Waze_merged_df['ts'], errors='coerce')\n",
        "\n",
        "# Create a new 'hour' column by extracting the hour from the 'ts' column\n",
        "Waze_merged_df['hour'] = Waze_merged_df['ts'].dt.hour.astype(str).str.zfill(2) + \":00\"\n",
        "# We will now collapse the dataset, aggregating by hour and binned latitude-longitude to get the count of alerts\n",
        "collapsed_df = (\n",
        "    Waze_merged_df.groupby(['hour', 'binned_lat_lon'])\n",
        "    .size()\n",
        "    .reset_index(name='alert_count')\n",
        ")\n",
        "\n",
        "# Sort the dataset by hour and alert count (descending)\n",
        "collapsed_df = collapsed_df.sort_values(by=['hour', 'alert_count'], ascending=[True, False])\n",
        "\n",
        "# Get the top 10 locations per hour\n",
        "top_alerts_by_hour = collapsed_df.groupby('hour').head(10)\n",
        "\n",
        "# Check how many rows this dataset has\n",
        "print(f\"The collapsed dataset has {top_alerts_by_hour.shape[0]} rows.\")\n",
        "\n",
        "# Save the collapsed dataset as 'top_alerts_map_byhour.csv'\n",
        "output_file = os.path.join('/Users/samarnegahdar/Desktop/untitled folder/pset-VI/top_alerts_map_byhour', 'top_alerts_map_byhour.csv')\n",
        "top_alerts_by_hour.to_csv(output_file, index=False)\n",
        "\n",
        "print(f\"Collapsed dataset saved as {output_file}\")"
      ],
      "id": "9df0b219",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "c. \n"
      ],
      "id": "c99fd800"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "selected_hours= ['11:00', '14:00', '22:00']\n",
        "##the jam-heavy df does not have hour column so I will add it:\n",
        "jam_heavy_df['hour']= Waze_merged_df['hour']\n",
        "# Define a function to create the plot for a given hour\n",
        "def create_hourly_plot(hour):\n",
        "    # Filter data for the selected hour\n",
        "    hour_data = jam_heavy_df[jam_heavy_df['hour'] == hour]\n",
        "\n",
        "    # Aggregate the data to find the number of alerts for each binned_lat_lon\n",
        "    top_alerts = (\n",
        "        hour_data.groupby('binned_lat_lon')\n",
        "        .size()\n",
        "        .reset_index(name='alert_count')\n",
        "        .sort_values(by='alert_count', ascending=False)\n",
        "        .head(10)\n",
        "    )\n",
        "\n",
        "    # Split the 'binned_lat_lon' into latitude and longitude\n",
        "    top_alerts[['latitude', 'longitude']] = pd.DataFrame(\n",
        "        top_alerts['binned_lat_lon'].tolist(),\n",
        "        index=top_alerts.index\n",
        "    )\n",
        "\n",
        "    # Create the scatter plot\n",
        "    scatter_plot_hourly = alt.Chart(top_alerts).mark_circle().encode(\n",
        "        x=alt.X('longitude:Q', title='Longitude', scale=alt.Scale(domain=[-87.8, -87.4])),\n",
        "        y=alt.Y('latitude:Q', title='Latitude', scale=alt.Scale(domain=[41.8, 42.0])),\n",
        "        size=alt.Size('alert_count:Q', title='Number of Alerts', legend=alt.Legend(title=\"Alert Count\")),\n",
        "        tooltip=['latitude:Q', 'longitude:Q', 'alert_count:Q']\n",
        "    ).properties(\n",
        "        title=f'Top 10 Locations for Jam - Heavy Traffic Alerts at {hour}',\n",
        "        width=600,\n",
        "        height=400\n",
        "    ).project(type=\"identity\", reflectY=True)\n",
        "\n",
        "\n",
        "\n",
        "    # Layer the scatter plot on top of the base map\n",
        "    layered_chart_3C = base_map + scatter_plot_hourly\n",
        "\n",
        "    return layered_chart_3C\n",
        "\n",
        "# Generate and display the plots for the selected hours\n",
        "for hour in selected_hours:\n",
        "    plot = create_hourly_plot(hour)\n",
        "    plot.display()"
      ],
      "id": "92e1a60a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2.\n",
        "a.\n",
        "![Hourly alerts](dashboard_hourly.png){width=70% align=\"center\"}\n",
        "\n",
        "b.\n",
        "![Minor Accident alerts](Minor_accident.png){width=70% align=\"center\"}\n",
        "\n",
        "c. I would like to remind again that you asked the wrong combination (if we wanted to see road closed- hazard-construction we needed subcategory and sub-subcategory) but now is too late for me to go back and your instructions were wrong so although I know I should look at the sub-category and sub-sub-category I will look at road-close- unclassified for morning and night. \n",
        "\n",
        "![road closed morning](road_closed_un_AM.png){width=70% align=\"center\"}\n",
        "\n",
        "\n",
        "![road closed night](road_closed_un_PM.png){width=70% align=\"center\"}\n",
        "if you take a look at the maps, you will see how most constructions take place at night \n",
        "\n",
        "\n",
        "# App #3: Top Location by Alert Type and Hour Dashboard (20 points){-}\n",
        "\n",
        "1. \n",
        "\n",
        "\n",
        "a. \n",
        "I don't think it is a good idea to do so since the range needs to be flexible while collapsing the date takes away the flexibility. still I think it would not be easy on shiny if we leave the range super open so maybe we can do a combination? where the hours will still be whole numbers but they range is defined within those hours. \n",
        "\n",
        "b. \n"
      ],
      "id": "868cdad7"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Ensure the 'hour' column is in the correct format\n",
        "jam_heavy_df['hour'] = pd.to_datetime(jam_heavy_df['hour'], errors='coerce').dt.strftime('%H:%M')\n",
        "\n",
        "# Filter for the time range 6AM-9AM and for a specific type-subtype combination (e.g., 'Jam - Heavy Traffic')\n",
        "filtered_df = jam_heavy_df[jam_heavy_df['hour'].isin(['06:00', '07:00', '08:00', '09:00'])]\n",
        "filtered_df = filtered_df[filtered_df['type_subtype'] == 'Jam - Traffic']  # Filter for the specific type-subtype\n",
        "\n",
        "# If 'alert_count' does not exist, create it (assuming we're counting occurrences of 'binned_lat_lon')\n",
        "if 'alert_count' not in filtered_df.columns:\n",
        "    filtered_df['alert_count'] = filtered_df.groupby('binned_lat_lon')['binned_lat_lon'].transform('count')\n",
        "\n",
        "# Aggregate alert counts by type_subtype and binned_lat_lon\n",
        "top_locations = (\n",
        "    filtered_df.groupby(['type_subtype', 'binned_lat_lon'])['alert_count']\n",
        "    .sum()\n",
        "    .reset_index()\n",
        "    .sort_values(by='alert_count', ascending=False)\n",
        "    .head(10)\n",
        ")\n",
        "\n",
        "# Check if 'binned_lat_lon' contains valid tuples (latitude, longitude)\n",
        "top_locations = top_locations[top_locations['binned_lat_lon'].apply(lambda x: isinstance(x, tuple) and len(x) == 2)]\n",
        "\n",
        "# If no valid locations, print a warning and exit\n",
        "if top_locations.empty:\n",
        "    print(\"Warning: No valid locations found for the selected type_subtype.\")\n",
        "else:\n",
        "    # Split 'binned_lat_lon' into latitude and longitude columns\n",
        "    top_locations[['latitude', 'longitude']] = pd.DataFrame(\n",
        "        top_locations['binned_lat_lon'].tolist(),\n",
        "        index=top_locations.index\n",
        "    )\n",
        "\n",
        "# Define latitude and longitude ranges for the map (for Chicago)\n",
        "lat_range = [41.8, 42.0]  \n",
        "lon_range = [-87.8, -87.4]\n",
        "\n",
        "# Create the base map using Altair with specified latitude and longitude ranges\n",
        "base_map_hourly_jam = alt.Chart(geo_data).mark_geoshape(\n",
        "    fill='lightgray',  # Fill color for the map\n",
        "    stroke='white'     # Border color for neighborhoods\n",
        ").project(\n",
        "    type='mercator',\n",
        "    scale=70000,  # Adjust scale to zoom in to the specific region # Center the map on Chicago's approximate coordinates\n",
        ").properties(\n",
        "    width=600,\n",
        "    height=400\n",
        ").encode(\n",
        "    longitude='longitude:Q',\n",
        "    latitude='latitude:Q'\n",
        ")\n",
        "\n",
        "# Adjust the base map's latitude and longitude range (domain)\n",
        "base_map_hourly_jam = base_map_hourly_jam.encode(\n",
        "    x=alt.X('longitude:Q', scale=alt.Scale(domain=lon_range)),  # Longitude domain\n",
        "    y=alt.Y('latitude:Q', scale=alt.Scale(domain=lat_range))    # Latitude domain\n",
        ")\n",
        "\n",
        "# Create the scatter plot for the top 10 locations\n",
        "scatter_plot = alt.Chart(top_locations).mark_circle().encode(\n",
        "    x=alt.X('longitude:Q', title='Longitude', scale=alt.Scale(domain=lon_range)),  # Longitude domain\n",
        "    y=alt.Y('latitude:Q', title='Latitude', scale=alt.Scale(domain=lat_range)),  # Latitude domain\n",
        "    size=alt.Size('alert_count:Q', title='Alert Count', legend=alt.Legend(title=\"Alert Count\")),\n",
        "    color=alt.Color('alert_count:Q', title='Alert Count', scale=alt.Scale(scheme='viridis')),\n",
        "    tooltip=['latitude:Q', 'longitude:Q', 'alert_count:Q']\n",
        ").properties(\n",
        "    title='Top 10 Locations for Jam - Heavy Traffic (6AM-9AM)',\n",
        "    width=600,\n",
        "    height=400\n",
        ")\n",
        "\n",
        "# Create the layered map by adding the scatter plot to the base map\n",
        "layered_chart_hourly_jam = base_map + scatter_plot\n",
        "\n",
        "# Show the layered map\n",
        "layered_chart_hourly_jam.display()"
      ],
      "id": "2213670a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "2. \n",
        "\n",
        "a. \n",
        "\n",
        "![Traffic jam with an hourly range](jam_traffic_hourly_dash.png){width=50% align=\"center\"}\n",
        "\n",
        "\n",
        "b. \n",
        "I used the same settings for part a so did not include the same photo twice\n",
        "    \n",
        "3. \n",
        "\n",
        "\n",
        "a.\n",
        "    \n",
        "\n",
        "![dashboard with a switch botton](Toggle%20switch.png){width=50% align=\"center\"}\n",
        "\n",
        "the possible values are true and false: true means the switch is on (meaning the user can choose a range) and False means the switch is off (the user will only be able to select one hour)\n",
        "\n",
        "b. \n",
        "![Toggle switch off](Toggle%20switch.png){width=50% align=\"center\"}\n",
        "\n",
        "![Toggle switch on](Toggle%20on.png){width=50% align=\"center\"}\n",
        "\n",
        "c. \n",
        "![sample of switch botton use](Toggle_sample1.png){width=50% align=\"center\"}\n",
        "\n",
        "![sample of switch botton use](Toggle_sample2.png.png){width=50% align=\"center\"}\n",
        "\n",
        "d.\n",
        "\n",
        "first thing we do is to change the slider range form 24 hours to 12 hours and then when the user chooses like 5 (which can mean both AM and PM) then in the server part we will now group by alert type and AM/PM \n",
        "then when plotting the scatterplot we need to add two conditions (size of marks by number of alerts and their color based on the AM/PM categorization)"
      ],
      "id": "8c607d79"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "/Users/samarnegahdar/Desktop/untitled folder/pset-VI/venv_pset6/share/jupyter/kernels/python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}